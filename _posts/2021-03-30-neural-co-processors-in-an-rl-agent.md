---
layout: post
title: Neural Co-processors in an RL Agent
---

Last quarter,  I conducted research on how the components of a [neural co-processor](https://www.sciencedirect.com/science/article/pii/S0959438818301843) can be incorporated in a reinforcment learning (RL) framework. For some basic background knowledge, a neural co-processor is a bidirectional brain-computer interface that can theoretically restore or augment brain function by jointly optimizing the cost function with the nervous system. The goal of this project is to demonstrate the efficacy of neural co-processors in a damaged RL agent.

For the RL framework, I used the [Deep Deterministic Policy Gradient](https://arxiv.org/abs/1509.02971) (DDPG) algorithm to train an agent in the OpenAI [BipedalWalker environment](https://gym.openai.com/envs/BipedalWalker-v2/). DDPG is a model-free off-policy actor-critic algorithm. The important thing to know is that the "Actor" part of the algorithm acts as a simplistic brain which takes as input the state of the environment and outputs the action. The Actor is just a feedforward neural network, but I may update this to a recurrent neural network (RNN) in the future since it's a better representation of the brain.

In this implementation, the Actor, which fully represents the agent, is a feedforward neural network with two hidden layers. The first hidden layer (H1) has 400 neurons, and the second hidden layer (H2) has 16 neurons. The number of states in the environment is 24, and the number of actions is 4. Neural co-processors consist of two components: 1) Co-processor Network and 2) Emulator Network. The goal of the Co-processor Network (CPN) is to learn how to stimulate a part of the brain to restore damage. The goal of the Emulator Network (EN) is to emulate biological feedback. In tandem, the EN is used to calculate the biological error which backpropagates through the CPN. The idea behind having the EN is that the CPN will never be able to update its weights by learning directly from biological feedback. Thus, the EN is there to replicate biological feedback as closely as possible.

![Actor](/assets/actor.png)

An artificial neural network is obviously an extremely simplified version of the brain, but it's still useful to draw some comparisons. In this model, the area of stimulation is H2, and the area of damage is H1. I damaged the model by freezing 96% of the neurons in H1, thereby reducing the number of active neurons from 400 to 16. The EN's job is to emulate biological feedback, so I trained the EN by learning the weights from H2 to the action in order to produce a surrogate action. I then trained the CPN to stimulate H2, but instead of using the real action as feedback, I used the surrogate action generated by the EN.

In summary, the following things were done to implement neural co-processors in a damaged agent:
1. I trained a fully functioning agent. The agent is able to walk perfectly well. **See Figure A**.
2. I damaged H1 of the fully functioning agent by freezing 96% of the neurons in H1. I continued to train the damaged agent to see how much it could recover without the neural co-processor. Note that the training curve is much more stable because less weights are being learned. The agent appears to be severely disabled. **See Figure B**.
3. I trained the EN by learning the weights from H2 to the action. **See Figure D**.
4. I trained the CPN using the surrogate action from the EN instead of the real action. The output of the CPN is appended to the output of the damaged layer (H1). As expected, the agent is now able to walk normally again with assistance from the CPN.  **See Figure C**.

|**Figure A**
|-------------------------|-------------------------
|![Actor](/assets/actor.gif)|![Graph](/assets/functioning_graph.png)|                 

|**Figure B**
|-------------------------|-------------------------
|![Damaged Actor](/assets/damaged-actor.gif)|![Graph](/assets/damaged_graph.png)|   

|**Figure C**
|-------------------------|-------------------------
|![Recovered Actor](/assets/recovered-actor.gif)|![Graph](/assets/recovered_graph.png)| 

|**Figure D**
|-------------------------
|![Graph](/assets/EN.png)|

The gifs show a simulation of the best set of weights learned in the training sequence. 

The graphs (A-C) plot the average reward over every 5 episodes (however many timesteps that is). 

All my code is [here](https://github.com/cjto2000/DDPG-PyTorch).